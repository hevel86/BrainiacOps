# Talos Node Management

This document provides comprehensive guidance for managing the Talos Linux cluster `talos-rao`, including adding new nodes, understanding the configuration structure, and performing upgrades.

## Table of Contents

- [Cluster Overview](#cluster-overview)
- [Prerequisites](#prerequisites)
- [Configuration Files Structure](#configuration-files-structure)
- [Understanding Factory Images and Installer URLs](#understanding-factory-images-and-installer-urls)
- [Adding a New Control Plane Node](#adding-a-new-control-plane-node)
- [System Configuration](#system-configuration)
- [Verification and Health Checks](#verification-and-health-checks)
- [Troubleshooting](#troubleshooting)
- [Upgrading a Node](#upgrading-a-node)
- [Automated Upgrades with Tuppr](#automated-upgrades-with-tuppr)

## Cluster Overview

**Cluster Name**: talos-rao

**Current Nodes**:
- brainiac-00 (10.0.0.34) - Control Plane
- brainiac-01 (10.0.0.35) - Control Plane
- brainiac-02 (10.0.0.36) - Control Plane

**Cluster Configuration**:
- Talos Version: v1.11.5
- Kubernetes Version: v1.34.1
- Cluster Endpoint (VIP): https://10.0.0.30:6443
- Network Interface: enp2s0f1np1 (DHCP with VIP)
- Pod Network: 10.244.0.0/16
- Service Network: 10.96.0.0/12
- Domain: cluster.local
- Control planes are allowed to schedule workloads

## Prerequisites

Before adding a new node or managing the cluster, ensure you have the following:

### Required Tools

This project uses [mise](https://mise.jdx.dev/) for tool version management. All required tools are defined in `.mise.toml`:

- **talhelper** v3.0.39 - Generates Talos machine configurations from simplified YAML
- **talosctl** v1.11.3 - Talos CLI for cluster management
- **kubectl** - Kubernetes CLI
- **SOPS** v3.11.0 - For encrypting/decrypting secrets
- **age** v1.2.1 - Encryption tool used with SOPS

To install all tools, run:
```bash
mise install
```

### Environment Setup

The following environment variables should be configured (automatically set by mise):

```bash
export KUBECONFIG=~/.kube/config
export TALOSCONFIG=~/.talos/talosconfig
export SOPS_AGE_KEY_FILE=~/.config/sops/age/keys.txt
```

### Age/SOPS Key Setup

To decrypt the `talsecret.sops.yaml` file, you need the age private key:

1. Ensure your age key is located at `~/.config/sops/age/keys.txt`
2. The public key should be referenced in `.sops.yaml`
3. Test decryption: `sops -d talos/talsecret.sops.yaml`

### Hardware Requirements

Nodes in this cluster are configured for Intel-based hardware with:

- **System Disk**: NVMe disk <= 600GB for OS installation
- **Storage Disks**: Disks >= 1.5TB (automatically provisioned for Longhorn storage)
- **Network Interface**: enp2s0f1np1 (10GbE interface)
- **DHCP**: Available on the network interface
- **UPS**: Access to UPS monitoring at `ups1500@batterypi.torquasmvo.internal` (optional)

### Network Requirements

- **Node IP Addresses**: Static IPs or DHCP reservations (10.0.0.34, 10.0.0.35, 10.0.0.36, etc.)
- **Virtual IP (VIP)**: 10.0.0.30 - shared across control plane nodes for HA
- **Network Access**: Nodes must be able to reach each other and the internet
- **DNS**: Proper hostname resolution for the cluster

## Configuration Files Structure

Understanding the configuration file structure is crucial for managing the cluster:

### Source Configuration Files (Version Controlled)

- **`talconfig.yaml`** - Main Talhelper configuration file that defines:
  - Cluster-wide settings (network, versions, endpoints)
  - Node definitions (hostnames, IPs, roles)
  - Patches and extensions
  - System configuration

- **`talsecret.sops.yaml`** - Encrypted secrets file containing:
  - Cluster secrets
  - Node tokens
  - Certificates and keys
  - Encrypted with age/SOPS

- **`.sops.yaml`** - SOPS configuration defining:
  - Age public keys for encryption
  - Encryption rules for different file types

### Generated Configuration Files (NOT Version Controlled)

These files are generated by `talhelper genconfig` and stored in `clusterconfig/`:

- **`talos-rao-brainiac-00.yaml`** - Machine config for brainiac-00
- **`talos-rao-brainiac-01.yaml`** - Machine config for brainiac-01
- **`talos-rao-brainiac-02.yaml`** - Machine config for brainiac-02
- **`talosconfig`** - Talos client configuration for talosctl
- **`talconfig.json`** - Intermediate JSON configuration

**Important**: Generated files in `clusterconfig/` contain sensitive data and are gitignored. Never commit these files to version control.

### Configuration Workflow

```
talconfig.yaml + talsecret.sops.yaml
          ↓
    talhelper genconfig
          ↓
clusterconfig/*.yaml (machine configs)
          ↓
    talosctl apply-config
          ↓
    Talos Node Configuration
```

## Understanding Factory Images and Installer URLs

Talos supports two types of installer images, and understanding the difference is critical to avoid losing system extensions during installation.

### Base Installer vs. Factory Installer

**Base Installer** (`ghcr.io/siderolabs/installer`):
- Generic Talos installer without any system extensions
- Smaller image size
- Extensions must be added after installation via upgrade

**Factory Installer** (`factory.talos.dev/installer/<schematic-id>`):
- Custom-built installer with specific system extensions baked in
- Extensions persist through installation and reboot
- Larger image but eliminates need for post-install upgrade

### How Schematic IDs Work

A **schematic ID** is a unique hash that identifies a specific set of system extensions. For this cluster, the schematic ID is:

```
284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc
```

This schematic includes all 9 system extensions defined in `talconfig.yaml`:
- siderolabs/i915
- siderolabs/intel-ice-firmware
- siderolabs/intel-ucode
- siderolabs/iscsi-tools
- siderolabs/mei
- siderolabs/nut-client
- siderolabs/nvme-cli
- siderolabs/thunderbolt
- siderolabs/util-linux-tools

### Configured Installer Image

This cluster is configured to use the factory installer with extensions:

```yaml
# In talconfig.yaml
talosImageURL: factory.talos.dev/installer/284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc
```

When `talhelper genconfig` runs, it embeds this factory URL into the generated machine configs. When you apply the config with `talosctl apply-config`, Talos installs using the factory image, ensuring all extensions are present from the first boot.

### Finding Your Schematic ID

You can find your schematic ID in two places:

1. **Factory ISO URL**: The schematic ID is in the URL you use to download the boot ISO from factory.talos.dev
2. **Generated machine configs**: Look for the `image:` field in files like `clusterconfig/talos-rao-brainiac-02.yaml`

### Common Pitfall: Extensions Disappearing After Installation

**Symptom**: You boot from a factory ISO with extensions, apply the machine config, but after the first reboot the extensions are gone and you need to run `talosctl upgrade` to get them back.

**Cause**: The `talosImageURL` in `talconfig.yaml` is set to the base installer (`ghcr.io/siderolabs/installer`) instead of the factory installer with your schematic ID.

**Solution**: Ensure `talconfig.yaml` uses the factory installer URL:

```yaml
controlPlane:
  talosImageURL: factory.talos.dev/installer/284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc
```

Then regenerate machine configs with `talhelper genconfig` and apply the updated configuration.

## Adding a New Control Plane Node

This section documents the complete workflow for adding a new control plane node to the cluster. This example uses `brainiac-02` (10.0.0.36) as the new node.

### Why Add Another Control Plane Node?

Adding additional control plane nodes provides:
- High availability for the Kubernetes API
- Safe maintenance windows for upgrades without quorum loss
- Distributed etcd for improved reliability

### Step 1: Update talconfig.yaml

Edit `talos/talconfig.yaml` and add the new node to the `nodes` section:

```yaml
nodes:
  # Existing nodes...
  - hostname: brainiac-02
    ipAddress: 10.0.0.36
    controlPlane: true
    installDiskSelector:
      size: "<= 600GB"
      type: "nvme"
```

**Note**: The `installDiskSelector` uses disk selectors instead of hardcoded paths, allowing Talos to automatically choose the appropriate system disk.

### Step 2: Update API Server Certificate SANs

In the same `talconfig.yaml` file, add the new node's IP address to the `additionalApiServerCertSans` list under the `controlPlane` section:

```yaml
controlPlane:
  patches:
    - |-
      cluster:
        apiServer:
          certSANs:
            - 10.0.0.30  # VIP
            - 10.0.0.34  # brainiac-00
            - 10.0.0.35  # brainiac-01
            - 10.0.0.36  # brainiac-02 (NEW)
```

This ensures the API server certificate is valid for connections to the new node.

### Step 3: Generate Machine Configurations

Use Talhelper to generate the machine configuration files:

```bash
cd talos
talhelper genconfig
```

This command:
1. Reads `talconfig.yaml` and `talsecret.sops.yaml`
2. Decrypts secrets using your age key
3. Generates individual machine configs in `clusterconfig/`
4. Creates a new `talosconfig` file for talosctl
5. Embeds the factory installer URL (with schematic ID) into each machine config

**Important**: Because `talosImageURL` in `talconfig.yaml` points to the factory installer with extensions, the generated machine configs will instruct Talos to install using that factory image. This ensures system extensions persist through installation without requiring a post-install upgrade.

**Verify the generated configuration**:
```bash
ls -la clusterconfig/
# Should see: talos-rao-brainiac-02.yaml
```

You can inspect the generated config (be careful, it contains secrets):
```bash
less clusterconfig/talos-rao-brainiac-02.yaml
```

### Step 4: Prepare the Physical Hardware

1. **Boot with Talos Installer**: Boot the new node using the Talos installer image. You can:
   - Use a USB drive with the Talos installer
   - PXE boot the installer
   - Boot from the Talos ISO

2. **Verify Network Connectivity**: Ensure the node receives an IP address via DHCP and can be reached from your workstation:
   ```bash
   ping 10.0.0.36
   ```

3. **Verify Disk Layout**: Check the available disks before applying the configuration to ensure the correct disk will be selected for installation:
   ```bash
   talosctl get disks --nodes 10.0.0.36 --insecure
   ```

   Look for:
   - **System disk**: An NVMe disk <= 600GB (e.g., 256GB-512GB) will be selected for OS installation
   - **Storage disks**: Any disks >= 1.5TB will be automatically provisioned for Longhorn storage
   - **Unwanted disks**: If you see any disks that shouldn't be in the system, power down and physically remove them before proceeding

   Example output:
   ```
   nvme0n1   500 GB    WD Red SN700          ← Will be used for system install
   nvme2n1   2.0 TB    Samsung SSD 990 PRO   ← Will be used for Longhorn
   ```

### Step 4b: Remove MetalLB-Blocking Node Label (IMPORTANT)

**⚠️ Critical for LoadBalancer Services**: Talos automatically adds the `node.kubernetes.io/exclude-from-external-load-balancers` label to control plane nodes in the generated configs. This label prevents MetalLB from using these nodes to announce LoadBalancer IPs, breaking external access to services.

Since this cluster uses `allowSchedulingOnControlPlanes: true` and has no dedicated worker nodes, you **must** comment out this label before applying the configuration.

**Edit the generated config** (`clusterconfig/talos-rao-brainiac-02.yaml`) and find the `nodeLabels` section (around line 48):

```yaml
machine:
  nodeLabels:
    # node.kubernetes.io/exclude-from-external-load-balancers: ""  ← COMMENT THIS OUT
```

**Why this is necessary**:
- MetalLB L2 mode uses nodes to announce LoadBalancer IPs via ARP
- The exclusion label tells MetalLB not to use these nodes
- Without this change, LoadBalancer services (Portainer, ArgoCD, Longhorn UI, etc.) won't be accessible via their external IPs
- This label is added by default for control plane nodes but doesn't apply to clusters where control planes also run workloads

### Step 5: Apply the Machine Configuration

Apply the generated configuration to the new node. Since this is a fresh install, use the `--insecure` flag (the node doesn't trust the cluster CA yet):

```bash
talosctl apply-config \
  --insecure \
  --nodes 10.0.0.36 \
  --file clusterconfig/talos-rao-brainiac-02.yaml
```

**What happens next**:
1. Talos installs to the selected disk (based on installDiskSelector)
2. The node reboots into the installed system
3. The node joins the etcd cluster
4. Kubernetes components start on the new control plane
5. The node becomes available for scheduling workloads

**Expected timeline**:
- Initial configuration: 1-2 minutes
- Installation and reboot: 3-5 minutes
- Node ready: 2-3 minutes after reboot
- Total: ~10 minutes

### Step 6: Monitor the Bootstrap Process

Watch the node bootstrap:

```bash
# Watch Talos service status
talosctl -n 10.0.0.36 services

# Follow the logs
talosctl -n 10.0.0.36 dmesg -f

# Watch Kubernetes node status
kubectl get nodes -w
```

Wait for the node to appear as `Ready` in kubectl output.

### Step 7: Verify System Extensions

After the node is running, verify that all system extensions are present (confirming the factory installer worked correctly):

```bash
talosctl -n 10.0.0.36 get extensions
```

You should see all 9 system extensions listed:
- siderolabs/i915
- siderolabs/intel-ice-firmware
- siderolabs/intel-ucode
- siderolabs/iscsi-tools
- siderolabs/mei
- siderolabs/nut-client
- siderolabs/nvme-cli
- siderolabs/thunderbolt
- siderolabs/util-linux-tools

**Important**: If extensions are missing at this stage, it means the factory installer URL in `talconfig.yaml` is not configured correctly. See the [Extensions Disappearing After Installation](#extensions-disappearing-after-installation) troubleshooting section.

## System Configuration

This section documents the system-level configuration applied to all control plane nodes.

### System Extensions

All control plane nodes have the following Talos system extensions installed:

- **siderolabs/i915** - Intel integrated GPU drivers (for media transcoding/Plex)
- **siderolabs/intel-ice-firmware** - Intel Ethernet firmware
- **siderolabs/intel-ucode** - Intel CPU microcode updates
- **siderolabs/iscsi-tools** - iSCSI initiator for network storage
- **siderolabs/mei** - Intel Management Engine Interface
- **siderolabs/nut-client** - Network UPS Tools client for UPS monitoring
- **siderolabs/nvme-cli** - NVMe management utilities
- **siderolabs/thunderbolt** - Thunderbolt support
- **siderolabs/util-linux-tools** - Additional Linux utilities

### Storage Configuration

#### System Disk Selection

System installation uses disk selectors instead of hardcoded paths:

```yaml
installDiskSelector:
  size: "<= 600GB"
  type: "nvme"
```

This allows Talos to automatically select the appropriate NVMe system disk (typically 512GB).

#### Longhorn Storage Provisioning

Larger disks are automatically configured for Longhorn distributed storage:

- **Disk Selection**: Disks >= 1.5TB that are not system disks
- **Mount Point**: `/var/mnt/longhorn` (bind-mounted with rshared propagation)
- **Maximum Size**: 2TB per disk
- **Purpose**: Provides persistent storage for Kubernetes workloads

Longhorn volumes are mounted with the `rshared` propagation mode to allow proper volume sharing between containers.

### Network UPS Tools (NUT) Configuration

All control plane nodes monitor the UPS for power events:

- **UPS Server**: `ups1500@batterypi.torquasmvo.internal`
- **Mode**: Secondary (monitor-only, does not control the UPS)
- **User**: `remote` (read-only access)

This allows nodes to safely shut down during extended power outages.

### Kubernetes Features

- **KubePrism**: Enabled on port 7445 for load-balanced API access
- **Host DNS**: Forwards DNS queries to kube-dns
- **Disk Quotas**: Enabled for storage management
- **Stable Hostnames**: Nodes use consistent hostnames
- **Default Seccomp**: Enabled for improved security
- **RBAC**: Enabled for access control

## Verification and Health Checks

After adding a node, perform these checks to ensure everything is working correctly.

### Check Node Status

Verify the node appears in Kubernetes:

```bash
kubectl get nodes
```

Expected output:
```
NAME          STATUS   ROLES           AGE   VERSION
brainiac-00   Ready    control-plane   10d   v1.34.1
brainiac-01   Ready    control-plane   10d   v1.34.1
brainiac-02   Ready    control-plane   5m    v1.34.1
```

### Check etcd Membership

Verify the node joined the etcd cluster:

```bash
talosctl -n 10.0.0.34 etcd members
```

You should see all three control plane nodes listed.

### Check Cluster Health

Verify overall cluster health:

```bash
# Check etcd health
talosctl -n 10.0.0.34 etcd status

# Check all pods are running
kubectl get pods -A

# Check cluster info
kubectl cluster-info

# Check component status
kubectl get componentstatuses
```

### Check Talos Services

Verify all Talos services are running on the new node:

```bash
talosctl -n 10.0.0.36 services
```

All services should show as "Running" and "Healthy".

### Check Storage Configuration

Verify Longhorn storage is properly configured:

```bash
# Check mounted volumes
talosctl -n 10.0.0.36 mounts | grep longhorn

# Check disk usage
talosctl -n 10.0.0.36 df
```

### Check System Extensions

Verify system extensions loaded correctly:

```bash
talosctl -n 10.0.0.36 get extensions
```

## Troubleshooting

Common issues when adding nodes and their solutions.

### Node Not Joining the Cluster

**Symptoms**: Node doesn't appear in `kubectl get nodes` after 10+ minutes

**Possible causes and solutions**:

1. **Network connectivity issues**
   ```bash
   # Test network from the node
   talosctl -n 10.0.0.36 get addresses

   # Ping the VIP
   talosctl -n 10.0.0.36 netstat
   ```

2. **etcd cluster not healthy**
   ```bash
   # Check etcd status on existing nodes
   talosctl -n 10.0.0.34 etcd status
   ```

3. **Configuration mismatch**
   - Verify the generated config was applied correctly
   - Check for errors in service logs:
   ```bash
   talosctl -n 10.0.0.36 logs kubelet
   talosctl -n 10.0.0.36 logs etcd
   ```

### Certificate Errors

**Symptoms**: "x509: certificate signed by unknown authority" or similar errors

**Solutions**:

1. Verify the node IP was added to `additionalApiServerCertSans`
2. Regenerate configs with `talhelper genconfig`
3. Reapply the configuration

### Extensions Disappearing After Installation

**Symptoms**: You boot from a factory ISO that includes system extensions, apply the machine config, but after the first reboot all extensions are gone. Running `talosctl -n <node-ip> get extensions` shows no extensions installed.

**Cause**: The `talosImageURL` in `talconfig.yaml` is configured to use the base installer (`ghcr.io/siderolabs/installer`) instead of the factory installer with your schematic ID.

**What's happening**:
1. You boot from factory ISO with extensions (e.g., `factory.talos.dev/metal-iso/...`)
2. You apply machine config via `talosctl apply-config`
3. Talos installs to disk using the installer image specified in the machine config
4. If the machine config references the base installer, it installs without extensions
5. After reboot, extensions are gone because they weren't in the installed image

**Solutions**:

1. **Update talconfig.yaml** to use the factory installer URL:
   ```yaml
   controlPlane:
     talosImageURL: factory.talos.dev/installer/284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc
   ```

2. **Regenerate machine configs**:
   ```bash
   cd talos
   talhelper genconfig
   ```

3. **Verify the generated config** includes the factory URL:
   ```bash
   grep "image:" clusterconfig/talos-rao-brainiac-02.yaml
   # Should show: factory.talos.dev/installer/...
   ```

4. **Reapply the configuration** to the node:
   ```bash
   talosctl apply-config --insecure --nodes 10.0.0.36 --file clusterconfig/talos-rao-brainiac-02.yaml
   ```

5. **Verify extensions after reboot**:
   ```bash
   talosctl -n 10.0.0.36 get extensions
   ```

**Prevention**: Always ensure `talosImageURL` in `talconfig.yaml` uses the factory installer URL with your schematic ID. See the [Understanding Factory Images and Installer URLs](#understanding-factory-images-and-installer-urls) section for more details.

### Disk Not Found

**Symptoms**: "no disk found matching install disk selector"

**Solutions**:

1. Check available disks:
   ```bash
   talosctl -n 10.0.0.36 --insecure disks
   ```

2. Adjust the `installDiskSelector` in `talconfig.yaml` if needed
3. Regenerate and reapply config

### Cannot Decrypt Secrets

**Symptoms**: "no age key found" or "failed to decrypt"

**Solutions**:

1. Verify age key exists: `cat ~/.config/sops/age/keys.txt`
2. Verify `SOPS_AGE_KEY_FILE` environment variable is set
3. Test decryption: `sops -d talos/talsecret.sops.yaml`

### Node Shows as NotReady

**Symptoms**: Node appears in kubectl but status is `NotReady`

**Solutions**:

1. Check CNI pods are running:
   ```bash
   kubectl get pods -n kube-system -l k8s-app=cilium
   ```

2. Check node conditions:
   ```bash
   kubectl describe node brainiac-02
   ```

3. Check kubelet logs:
   ```bash
   talosctl -n 10.0.0.36 logs kubelet
   ```

### Services Not Starting

**Symptoms**: Talos services show as not running or unhealthy

**Solutions**:

1. Check service status:
   ```bash
   talosctl -n 10.0.0.36 services
   ```

2. View service logs:
   ```bash
   talosctl -n 10.0.0.36 logs <service-name>
   ```

3. Check for system resource issues:
   ```bash
   talosctl -n 10.0.0.36 top
   ```

### LoadBalancer Services Not Accessible

**Symptoms**: MetalLB LoadBalancer IPs are assigned but not accessible from external machines (Longhorn UI, Portainer, ArgoCD, etc.)

**Cause**: The `node.kubernetes.io/exclude-from-external-load-balancers` label on control plane nodes prevents MetalLB from announcing IPs via ARP.

**Solutions**:

1. Check if the label exists on nodes:
   ```bash
   kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.labels.node\.kubernetes\.io/exclude-from-external-load-balancers}{"\n"}{end}'
   ```

2. If the label is present (shows empty value), remove it:
   ```bash
   kubectl label node brainiac-00 brainiac-01 brainiac-02 node.kubernetes.io/exclude-from-external-load-balancers-
   ```

3. Verify MetalLB is announcing services:
   ```bash
   kubectl logs -n metallb-system daemonset/metallb-speaker --tail=50 | grep serviceAnnounced
   ```

4. Clear ARP cache on your machine to pick up new announcements:
   ```bash
   sudo ip -s -s neigh flush all  # Linux
   arp -d  # Windows
   ```

**Prevention**: When applying new Talos configs or adding nodes, always comment out the `nodeLabels` section in the generated configs before applying (see Step 4b in Adding a New Control Plane Node).

## Upgrading a Node

Once the cluster is stable and has quorum (3+ control plane nodes), you can safely upgrade individual nodes.

### Prerequisites

- Cluster has multiple control plane nodes (3+ recommended)
- All nodes are healthy and etcd has quorum
- You have the target Talos version installer image URL

### Upgrade Process

Use the `talosctl upgrade` command, specifying the node to upgrade and the target installer image:

```bash
talosctl upgrade \
  --nodes 10.0.0.35 \
  --image factory.talos.dev/metal-installer/284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc:v1.11.5
```

Replace `10.0.0.35` with the IP address of the node you are upgrading.

**Note**: The installer image URL should include the factory schematic ID that matches your system extensions. Find your schematic ID in `talconfig.yaml` under `talosImageURL`.

### Upgrade Order

For control plane nodes, upgrade one at a time:

1. Upgrade brainiac-00, wait for it to become Ready
2. Upgrade brainiac-01, wait for it to become Ready
3. Upgrade brainiac-02, wait for it to become Ready

### Monitor the Upgrade

```bash
# Watch the node status
kubectl get nodes -w

# Check upgrade progress on the node
talosctl -n 10.0.0.35 dmesg -f
```

### Troubleshooting Upgrades

**If the upgrade fails or the node doesn't come back**:

1. Check node logs:
   ```bash
   talosctl -n 10.0.0.35 logs kubelet
   ```

2. If certificate issues occur, use `--insecure` flag:
   ```bash
   talosctl upgrade --nodes 10.0.0.35 --insecure --image <image-url>
   ```

3. If the node is completely unresponsive, you may need to apply the config again:
   ```bash
   talosctl apply-config --insecure --nodes 10.0.0.35 --file clusterconfig/talos-rao-brainiac-01.yaml
   ```

## Automated Upgrades with Tuppr

[Tuppr](https://github.com/home-operations/tuppr) is a Kubernetes controller that automates Talos and Kubernetes upgrades with built-in safety checks and observability. This cluster now includes tuppr for streamlined upgrade management.

### Benefits of Tuppr

- **Automated orchestration**: Sequential node-by-node upgrades with automatic health checks
- **Safety mechanisms**: Pre-upgrade validation using CEL expressions
- **Observability**: Prometheus metrics for monitoring upgrade progress
- **GitOps-friendly**: Upgrades tracked in Git via Custom Resources
- **Reduced manual effort**: No need to run `talosctl upgrade` for each node

### How Tuppr Works with This Cluster

Tuppr integrates seamlessly with your existing GitOps workflow:

1. **Renovate detects new Talos/Kubernetes releases** and creates a PR updating:
   - `talos/talconfig.yaml` (version definitions)
   - Tuppr upgrade resources in `kubernetes/infrastructure/tuppr/upgrades/`

2. **You review the PR**, checking release notes and compatibility

3. **Merge the PR** → Argo CD applies the upgrade resource to the cluster

4. **Tuppr orchestrates the upgrade**:
   - Validates cluster health
   - Upgrades node 1 (10.0.0.34), waits for it to become ready
   - Upgrades node 2 (10.0.0.35), waits for it to become ready
   - Upgrades node 3 (10.0.0.36), waits for it to become ready
   - Marks upgrade as complete

### Creating a Talos Upgrade

To upgrade Talos Linux (example: v1.11.5 → v1.11.6):

1. **Copy the template**:
   ```bash
   cp kubernetes/infrastructure/tuppr/upgrades/talos-upgrade-template.yaml.example \
      kubernetes/infrastructure/tuppr/upgrades/talos-v1.11.6.yaml
   ```

2. **Edit the file** and update:
   ```yaml
   metadata:
     name: upgrade-to-v1-11-6  # Update name
   spec:
     version: v1.11.6  # Target version
     # Ensure schematic ID matches talconfig.yaml!
     installer: factory.talos.dev/metal-installer/284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc:v1.11.6
   ```

3. **Update talconfig.yaml** to match:
   ```yaml
   talosVersion: v1.11.6
   talosImageURL: factory.talos.dev/installer/284a1fe...dcc:v1.11.6
   ```

4. **Commit both files together** in the same PR

5. **Argo CD syncs** the upgrade resource, tuppr starts the upgrade

### Creating a Kubernetes Upgrade

Similar process for Kubernetes upgrades:

```bash
cp kubernetes/infrastructure/tuppr/upgrades/kubernetes-upgrade-template.yaml.example \
   kubernetes/infrastructure/tuppr/upgrades/kubernetes-v1.34.2.yaml
```

Update `kubernetesVersion` in `talconfig.yaml` and the upgrade resource version to match.

### Monitoring Upgrade Progress

**View upgrade status**:
```bash
kubectl get talosupgrade -n system-upgrade
kubectl describe talosupgrade upgrade-to-v1-11-6 -n system-upgrade
```

**Watch Prometheus metrics** (if you have Prometheus configured):
- Tuppr exposes metrics on port 8080
- Track upgrade progress, node health, and completion status

**Check node status**:
```bash
kubectl get nodes -w
```

### Important Notes

**Schematic ID Preservation**:
- The schematic ID `284a1fe978ff4e6221a0e95fc1d01278bab28729adcb54bb53f7b0d3f2951dcc` defines your system extensions (i915, iscsi-tools, nut-client, etc.)
- **Always include this schematic** in the `installer` URL
- Version tags change (`:v1.11.5` → `:v1.11.6`), but schematic stays the same
- Using the base installer or wrong schematic will lose your extensions!

**Upgrade Constraints**:
- Only one `TalosUpgrade` resource can exist at a time
- Only one `KubernetesUpgrade` resource can exist at a time
- TalosUpgrade and KubernetesUpgrade cannot run concurrently

**Git-First Workflow**:
- Always update `talconfig.yaml` versions BEFORE creating upgrade resources
- Commit both changes together in the same PR
- This ensures Git remains the source of truth

**Post-Upgrade**:
- After tuppr completes an upgrade, optionally run `talhelper genconfig` to update generated configs for documentation purposes
- This isn't strictly necessary (configs are already applied), but keeps the `clusterconfig/` directory in sync

### Troubleshooting Tuppr Upgrades

**Upgrade stuck or failing**:
1. Check the upgrade resource status:
   ```bash
   kubectl describe talosupgrade <name> -n system-upgrade
   ```

2. View tuppr controller logs:
   ```bash
   kubectl logs -n system-upgrade deployment/tuppr -f
   ```

3. Verify Talos API access is enabled (required for tuppr):
   ```bash
   # Should show kubernetesTalosAPIAccess enabled
   talosctl -n 10.0.0.34 get machineconfig
   ```

**Health checks failing**:
- Review the `healthChecks` section in your upgrade resource
- Common checks: `talos.ready`, `kubernetes.ready`, `longhorn.ready`
- Ensure cluster is healthy before starting upgrade

**Rollback**:
- Delete the upgrade resource to stop progression:
  ```bash
  kubectl delete talosupgrade <name> -n system-upgrade
  ```
- Manually downgrade if needed using `talosctl upgrade` with the previous version

### Disabling Tuppr (Reverting to Manual Upgrades)

If you prefer manual upgrades:

1. Delete the tuppr Application in Argo CD:
   ```bash
   kubectl delete application tuppr -n argocd
   ```

2. Remove tuppr upgrade resources from Git:
   ```bash
   git rm kubernetes/infrastructure/tuppr/upgrades/*.yaml
   ```

3. Continue using manual `talosctl upgrade` commands as documented in [Upgrading a Node](#upgrading-a-node)

## Additional Resources

- [Talos Documentation](https://www.talos.dev/)
- [Talhelper Documentation](https://github.com/budimanjojo/talhelper)
- [SOPS Documentation](https://github.com/getsops/sops)
- [Longhorn Documentation](https://longhorn.io/docs/)

---

**Last Updated**: 2025-11-21
**Cluster Version**: Talos v1.11.5 / Kubernetes v1.34.1
