apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: nvidia-gpu-operator
  namespace: argocd # <-- The namespace where Argo CD is running
  # Add an annotation to instruct Argo CD on how to handle Helm hooks.
  # This prevents hooks from getting "stuck" during sync/upgrade operations.
  annotations:
    argocd.argoproj.io/hook-delete-policy: "BeforeHookCreation,HookSucceeded"
spec:
  project: default
  source:
    # The official NVIDIA Helm chart repository
    repoURL: 'https://helm.ngc.nvidia.com/nvidia'
    chart: gpu-operator
    targetRevision: v25.3.2 # <-- Pin to a specific version for consistency

    # --- This is where you put the configuration ---
    helm:
      values: |
        # This includes the critical fix we found for your k3s environment
        toolkit:
          env:
          - name: CONTAINERD_SOCKET
            value: /run/k3s/containerd/containerd.sock
          - name: CONTAINERD_CONFIG
            value: /var/lib/rancher/k3s/agent/etc/containerd/config.toml #I had this wrong initially
          - name: CONTAINERD_RUNTIME_DIR
            value: /run/k3s/containerd
          # Use 'systemd' mode and specify the 'k3s' service to ensure
          # containerd is reliably reloaded on your control-plane nodes.
          restartMode: systemd
          serviceName: k3s

        # Disable MIG manager for non-MIG GPUs (P1000/P2000)
        migManager:
          name: default-mig-parted-config
          enabled: false

        # For k3s, the kubelet root directory is non-standard. We must specify it
        # so that components like the device-plugin can find the kubelet socket.
        kubelet:
          root: /var/lib/rancher/k3s/server/agent

        # Increase the log level for the driver pods to assist with troubleshooting.
        # This will provide more detailed output if the driver fails to install.
        driver:
          logLevel: DEBUG

        # Since your nodes are also control-plane/master nodes, we need to add
        # tolerations so the operator's pods (like the driver daemonset) can run on them.
        tolerations:
        - key: "node-role.kubernetes.io/master"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"

        # By setting an empty nodeSelector, we ensure the operator's components
        # are not restricted to a specific node. This allows the operator to run
        # on all nodes with GPUs.
        nodeSelector: {}

        # Enable GPU time-slicing to allow multiple pods to share a single GPU.
        devicePlugin:
          # Increase log verbosity for the device-plugin to aid in debugging.
          logLevel: "debug"
          # Configure time-slicing using the chart's map/default structure.
          config:
            name: nvidia-device-plugin-config
            default: "time-sliced"
            map:
              time-sliced:
                version: v1
                flags:
                  migStrategy: none
                sharing:
                  timeSlicing:
                    resources:
                    - name: nvidia.com/gpu
                      replicas: 10

  # --- Where to deploy the operator in your cluster ---
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: gpu-operator # <-- The namespace for the GPU operator itself

  # --- Automated syncing policy ---
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - ServerSideApply=true
